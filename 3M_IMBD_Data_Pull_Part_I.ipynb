{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alortiz05/DDDS-Cohort-16-Projects/blob/main/3M_IMBD_Data_Pull_Part_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8ebELBpxHNq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV7421zwvOqR",
        "outputId": "e9dae6cd-082e-4f22-b544-f4c2e7063d08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.11/dist-packages (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install duckdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04bPqGCImXRm",
        "outputId": "75faa096-7552-40be-c28b-a829baac3fd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMDb file sizes (compressed):\n",
            "name.basics.tsv.gz: 272.99 MB\n",
            "title.akas.tsv.gz: 434.48 MB\n",
            "title.basics.tsv.gz: 197.72 MB\n",
            "title.crew.tsv.gz: 73.12 MB\n",
            "title.episode.tsv.gz: 47.50 MB\n",
            "title.principals.tsv.gz: 687.22 MB\n",
            "title.ratings.tsv.gz: 7.58 MB\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "imdb_files = [\n",
        "    'https://datasets.imdbws.com/name.basics.tsv.gz',\n",
        "    'https://datasets.imdbws.com/title.akas.tsv.gz',\n",
        "    'https://datasets.imdbws.com/title.basics.tsv.gz',\n",
        "    'https://datasets.imdbws.com/title.crew.tsv.gz',\n",
        "    'https://datasets.imdbws.com/title.episode.tsv.gz',\n",
        "    'https://datasets.imdbws.com/title.principals.tsv.gz',\n",
        "    'https://datasets.imdbws.com/title.ratings.tsv.gz'\n",
        "]\n",
        "\n",
        "print(\"IMDb file sizes (compressed):\")\n",
        "for url in imdb_files:\n",
        "    try:\n",
        "        response = requests.head(url, allow_redirects=True)\n",
        "        size_bytes = int(response.headers.get('Content-Length', 0))\n",
        "        size_mb = size_bytes / (1024 * 1024)\n",
        "        print(f\"{url.split('/')[-1]}: {size_mb:.2f} MB\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get size for {url}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZaW011kmX2y",
        "outputId": "0308e557-9dd8-428a-e8f4-b0077803e759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "//content/drive/MyDrive/Capstone Project/imdb_data/name.basics.tsv.gz already exists.\n",
            "//content/drive/MyDrive/Capstone Project/imdb_data/title.akas.tsv.gz already exists.\n",
            "//content/drive/MyDrive/Capstone Project/imdb_data/title.basics.tsv.gz already exists.\n",
            "//content/drive/MyDrive/Capstone Project/imdb_data/title.crew.tsv.gz already exists.\n",
            "//content/drive/MyDrive/Capstone Project/imdb_data/title.episode.tsv.gz already exists.\n",
            "//content/drive/MyDrive/Capstone Project/imdb_data/title.principals.tsv.gz already exists.\n",
            "//content/drive/MyDrive/Capstone Project/imdb_data/title.ratings.tsv.gz already exists.\n"
          ]
        }
      ],
      "source": [
        "import duckdb\n",
        "import os\n",
        "import requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "download_dir = \"//content/drive/MyDrive/Capstone Project/imdb_data\"  # Save directly to your Drive\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "imdb_files = [\n",
        "    'https://datasets.imdbws.com/name.basics.tsv.gz',\n",
        "    'https://datasets.imdbws.com/title.akas.tsv.gz',\n",
        "    'https://datasets.imdbws.com/title.basics.tsv.gz',\n",
        "    'https://datasets.imdbws.com/title.crew.tsv.gz',\n",
        "    'https://datasets.imdbws.com/title.episode.tsv.gz',\n",
        "    'https://datasets.imdbws.com/title.principals.tsv.gz',\n",
        "    'https://datasets.imdbws.com/title.ratings.tsv.gz'\n",
        "]\n",
        "\n",
        "for url in imdb_files:\n",
        "    file_name = os.path.join(download_dir, url.split('/')[-1])\n",
        "    if os.path.exists(file_name):\n",
        "        print(f\"{file_name} already exists.\")\n",
        "        continue\n",
        "    print(f\"Downloading {url} ...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(file_name, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "    print(f\"Saved to {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq18_Jq8uud2",
        "outputId": "971f0cca-1a9f-42ae-f5cd-a5959107ea86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBuY-hEtn-am"
      },
      "source": [
        "- did not use title_episode because we are looking at movies\n",
        "- made sure to not pull in multiple tconst\n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "9Z13Q3jvTk73",
        "outputId": "4a285d91-fe9d-4478-cef9-fc2415fa9a76"
      },
      "outputs": [
        {
          "ename": "ParserException",
          "evalue": "Parser Error: syntax error at or near \"combined\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-2618148572.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m# Step 3: Run the query with LIMIT inside\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Step 4: Fetch result into pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mParserException\u001b[0m: Parser Error: syntax error at or near \"combined\""
          ]
        }
      ],
      "source": [
        "\n",
        "# Connect to DuckDB in-memory (or specify a file for persistent DB)\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Ensure download_dir points to the location where files were saved\n",
        "# This should match the download_dir used in the previous cell\n",
        "download_dir = \"/content/drive/MyDrive/Capstone Project/imdb_data\"\n",
        "\n",
        "# Modify the query to use local file paths instead of URLs\n",
        "query = f\"\"\"\n",
        "WITH title_data AS (\n",
        "    SELECT -- these are all the columns I want to include from\n",
        "        tb.tconst,\n",
        "        tb.originalTitle,\n",
        "        tb.runtimeMinutes,\n",
        "        tb.startYear,\n",
        "        y.genre, --All Columns from title.basics\n",
        "        tr.averageRating,\n",
        "        tn.region\n",
        "    FROM read_csv_auto('{download_dir}/title.basics.tsv.gz', compression='gzip', sep='\\t', nullstr='\\\\N') tb\n",
        "\n",
        "    LEFT JOIN LATERAL (\n",
        "        SELECT UNNEST(STRING_SPLIT(tb.genres, ',')) AS genre\n",
        "    ) AS y ON TRUE\n",
        "\n",
        "    LEFT JOIN read_csv_auto('{download_dir}/title.crew.tsv.gz', compression='gzip', sep='\\t', nullstr='\\\\N') tc\n",
        "         ON tb.tconst = tc.tconst\n",
        "    LEFT JOIN read_csv_auto('{download_dir}/title.principals.tsv.gz', compression='gzip', sep='\\t', nullstr='\\\\N') tp\n",
        "        ON tb.tconst = tp.tconst\n",
        "    LEFT JOIN read_csv_auto('{download_dir}/title.ratings.tsv.gz', compression='gzip', sep='\\t', nullstr='\\\\N') tr\n",
        "        ON tb.tconst = tr.tconst\n",
        "    LEFT JOIN read_csv_auto('{download_dir}/title.akas.tsv.gz', compression='gzip', sep='\\t', nullstr='\\\\N') tn\n",
        "        ON tb.tconst = tn.titleID\n",
        "    WHERE tb.titleType = 'movie'\n",
        " ),\n",
        "\n",
        "name_data AS (\n",
        "    SELECT\n",
        "        nb.primaryName,\n",
        "        nb.PrimaryProfession,\n",
        "        g.tconst2,\n",
        "        h.Primary as primaryProfessionSplit\n",
        "    FROM read_csv_auto('{download_dir}/name.basics.tsv.gz', compression='gzip', sep='\\t', nullstr='\\\\N') nb,\n",
        "    LATERAL (SELECT UNNEST(STRING_SPLIT(nb.knownForTitles, ',')) AS tconst2) AS g,\n",
        "    LATERAL (SELECT UNNEST(STRING_SPLIT(nb.primaryProfession, ',')) AS Primary) AS h\n",
        ")\n",
        "\n",
        "combined AS (\n",
        "    SELECT\n",
        "        td.tconst,\n",
        "        td.originalTitle,\n",
        "        td.runtimeMinutes,\n",
        "        td.genre,\n",
        "        td.averageRating,\n",
        "        td.region,\n",
        "        nd.primaryName,\n",
        "        nd.primaryProfessionSplit,\n",
        "        td.startYear\n",
        "    FROM title_data td\n",
        "    LEFT JOIN name_data nd\n",
        "        ON td.tconst = nd.tconst2\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    tconst,\n",
        "    originalTitle,\n",
        "    runtimeMinutes,\n",
        "    genre,\n",
        "    averageRating,\n",
        "    region,\n",
        "    primaryName,\n",
        "    primaryProfessionSplit\n",
        "FROM combined\n",
        "ORDER BY startYear\n",
        "\n",
        "LIMIT 2000000\n",
        "\"\"\"\n",
        "\n",
        "# Step 3: Run the query with LIMIT inside\n",
        "con.execute(query)\n",
        "\n",
        "# Step 4: Fetch result into pandas\n",
        "df = con.fetchdf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA1Ol05767VV"
      },
      "outputs": [],
      "source": [
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zTXPRIhvuV1"
      },
      "outputs": [],
      "source": [
        "print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WziiPzClvw8_"
      },
      "outputs": [],
      "source": [
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF8xukSDwIJf"
      },
      "outputs": [],
      "source": [
        "print(df.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5Wo548BqHGH"
      },
      "outputs": [],
      "source": [
        "df.to_parquet(\"/content/drive/MyDrive/Capstone Project/imdb_ml_data2M.parquet\", index=False)\n",
        "print(\"✅ Data saved as Parquet to Google Drive.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klH-osVNrgY7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_parquet(\"//content/drive/MyDrive/Capstone Project/imdb_ml_data2M.parquet\")\n",
        "df.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bn55mBvm-PMT"
      },
      "outputs": [],
      "source": [
        "filtered_df2 = df[df['originalTitle'] == 'The Ring']\n",
        "filtered_df2.drop(columns=['averageRating']) #removing the rating column before using it to rest the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDBvMb00ktvm"
      },
      "outputs": [],
      "source": [
        "df_sample = df.sample(n=900000, random_state=42)\n",
        "\n",
        "# Step 3: Drop rows where target is missing\n",
        "df_ml = df_sample[df_sample['averageRating'].notna()]\n",
        "\n",
        "# Step 4: Define target and features\n",
        "target = 'averageRating'\n",
        "features = [\n",
        "    'runtimeMinutes',\n",
        "    'tconst',\n",
        "    'genre',\n",
        "    'originalTitle',\n",
        "    'category',\n",
        "    'primaryName'\n",
        "]\n",
        "df_ml = df_ml[features + [target]].dropna()\n",
        "\n",
        "# Step 5: Train/test split\n",
        "X = df_ml[features]\n",
        "y = df_ml[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Preprocessing and model pipeline\n",
        "categorical = ['tconst','genre', 'originalTitle', 'category', 'primaryName']\n",
        "numerical = ['runtimeMinutes']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', SimpleImputer(strategy='mean'), numerical),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical)\n",
        "])\n",
        "\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Step 7: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)\n",
        "print(\"RMSE:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"R^2 Score:\", r2_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaRt9VxlvyDa"
      },
      "outputs": [],
      "source": [
        "importances = model.named_steps['regressor'].feature_importances_\n",
        "feature_names = model.named_steps['preprocessor'].get_feature_names_out()\n",
        "importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "print(importance_df.sort_values(by='importance', ascending=False).head(10))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}